{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fridge Chatbot Prototype (Local Tiny Model)\n",
    "This notebook demonstrates a proof-of-concept AI chatbot that maintains a fridge state and can provide dinner suggestions. Uses a tiny local model (`flan-t5-small`) for CPU inference."
   ],
   "id": "9d6575a955076cea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Install Dependencies",
   "id": "f6ef149aa0aa3d1f"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-22T06:02:44.500217Z",
     "start_time": "2025-08-22T06:02:36.593129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install transformers torch sentencepiece\n",
    "!pip install gpt4all\n",
    "!pip install transformers accelerate bitsandbytes>0.37.0\n",
    "!pip install --upgrade accelerate\n",
    "!pip install huggingface_hub[hf_xet]\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\cook-bot\\.venv\\lib\\site-packages (4.55.3)\n",
      "Requirement already satisfied: torch in d:\\cook-bot\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: sentencepiece in d:\\cook-bot\\.venv\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in d:\\cook-bot\\.venv\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in d:\\cook-bot\\.venv\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sgsey\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sgsey\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\cook-bot\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\cook-bot\\.venv\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in d:\\cook-bot\\.venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\cook-bot\\.venv\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\cook-bot\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\cook-bot\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\cook-bot\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\cook-bot\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\cook-bot\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\cook-bot\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\cook-bot\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sgsey\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\cook-bot\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpt4all in d:\\cook-bot\\.venv\\lib\\site-packages (2.8.2)\n",
      "Requirement already satisfied: requests in d:\\cook-bot\\.venv\\lib\\site-packages (from gpt4all) (2.32.5)\n",
      "Requirement already satisfied: tqdm in d:\\cook-bot\\.venv\\lib\\site-packages (from gpt4all) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->gpt4all) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->gpt4all) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->gpt4all) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->gpt4all) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sgsey\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->gpt4all) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in d:\\cook-bot\\.venv\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\sgsey\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sgsey\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\sgsey\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in d:\\cook-bot\\.venv\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\cook-bot\\.venv\\lib\\site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in d:\\cook-bot\\.venv\\lib\\site-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\cook-bot\\.venv\\lib\\site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.7.0)\n",
      "Requirement already satisfied: requests in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\cook-bot\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\cook-bot\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\cook-bot\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\cook-bot\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\cook-bot\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sgsey\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\cook-bot\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in d:\\cook-bot\\.venv\\lib\\site-packages (0.34.4)\n",
      "Requirement already satisfied: filelock in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sgsey\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub[hf_xet]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in d:\\cook-bot\\.venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (1.1.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\sgsey\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\cook-bot\\.venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Import Libraries",
   "id": "25b65074a6e1c44b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:38:52.869265Z",
     "start_time": "2025-08-22T07:38:52.865668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from gpt4all import GPT4All\n",
    "from transformers import GenerationConfig"
   ],
   "id": "2fe1a99249af8c5e",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Load Model",
   "id": "6b19cc4851ea0887"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:34:31.009250Z",
     "start_time": "2025-08-22T07:34:29.746741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def resolve_model_dir(rel_path: str) -> Path:\n",
    "    p = Path(rel_path)\n",
    "    try:\n",
    "        return p.resolve(strict=True)\n",
    "    except FileNotFoundError:\n",
    "        # Try relative to the notebook/script directory if needed, or raise a clear error\n",
    "        raise FileNotFoundError(\n",
    "            f\"Local model directory not found at '{rel_path}'. \"\n",
    "            f\"Run from your project root or pass an absolute path.\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(Path().resolve())  # current working directory\n",
    "print((Path(\"src/models/gpt4all-j-bnb-4bit-smashed\")).resolve(strict=False))\n",
    "\n",
    "local_model_dir = Path(r\"D:\\Cook-Bot\\src\\models\\gpt4all-j-bnb-4bit-smashed\").resolve(strict=True)\n",
    "\n",
    "required_files = [\"config.json\", \"model.safetensors\"]\n",
    "missing = [f for f in required_files if not (local_model_dir / f).exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing required file(s) in '{local_model_dir}': {missing}\"\n",
    "    )\n",
    "\n",
    "use_bitsandbytes = False\n",
    "try:\n",
    "    import bitsandbytes as _  # noqa: F401\n",
    "    use_bitsandbytes = True\n",
    "except Exception:\n",
    "    use_bitsandbytes = False\n",
    "\n",
    "if not use_bitsandbytes:\n",
    "    raise RuntimeError(\n",
    "        \"The local model appears to be 4-bit bitsandbytes-quantized. \"\n",
    "        \"Load will fail without 'bitsandbytes'. Either:\\n\"\n",
    "        \"  - Use a non-quantized GPT-J checkpoint in the same folder, or\\n\"\n",
    "        \"  - Add 'bitsandbytes' to your environment.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Load tokenizer from HF repo (requires internet)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/gpt4all-j\")\n",
    "\n",
    "# Model load with offload to CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_dir,\n",
    "    device_map=None,\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,  # <-- allow CPU offload for modules that don't fit\n",
    "    trust_remote_code=True\n",
    ")"
   ],
   "id": "9c7128f553d6b37e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Cook-Bot\\src\\models\n",
      "D:\\Cook-Bot\\src\\models\\src\\models\\gpt4all-j-bnb-4bit-smashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Initialize Fridge State and Conversation Memory",
   "id": "f8c1e992d1134401"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:36:07.182604Z",
     "start_time": "2025-08-22T07:36:07.179673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fridge inventory\n",
    "fridge_state = {\n",
    "    \"eggs\": 4,\n",
    "    \"milk\": \"500ml\",\n",
    "    \"tomatoes\": 3,\n",
    "    \"cheese\": \"200g\"\n",
    "}\n",
    "\n",
    "# Conversation history (for sliding window memory)\n",
    "conversation_history = []\n",
    "SLIDING_WINDOW = 3  # last N messages"
   ],
   "id": "eafca0442ebe3241",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Helper Functions\n",
    "### 5.1 Update Fridge State (simple rule-based)"
   ],
   "id": "dde845599eee7393"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:36:09.132108Z",
     "start_time": "2025-08-22T07:36:09.127688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def update_fridge(user_input: str):\n",
    "    msg = None\n",
    "    user_input_lower = user_input.lower()\n",
    "    if \"eat\" in user_input_lower or \"used\" in user_input_lower:\n",
    "        if \"egg\" in user_input_lower and fridge_state.get(\"eggs\", 0) > 0:\n",
    "            # try to detect number\n",
    "            import re\n",
    "            match = re.search(r'(\\d+)', user_input_lower)\n",
    "            qty = int(match.group(1)) if match else 1\n",
    "            fridge_state[\"eggs\"] = max(fridge_state[\"eggs\"] - qty, 0)\n",
    "            msg = f\"Removed {qty} egg(s). You now have {fridge_state['eggs']} eggs.\"\n",
    "    if \"add\" in user_input_lower or \"buy\" in user_input_lower:\n",
    "        if \"egg\" in user_input_lower:\n",
    "            import re\n",
    "            match = re.search(r'(\\d+)', user_input_lower)\n",
    "            qty = int(match.group(1)) if match else 1\n",
    "            fridge_state[\"eggs\"] += qty\n",
    "            msg = f\"Added {qty} egg(s). You now have {fridge_state['eggs']} eggs.\"\n",
    "    return msg"
   ],
   "id": "31abdec59f5afce1",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 Build Prompt for Model",
   "id": "afd70611a7b35ef7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:36:11.060821Z",
     "start_time": "2025-08-22T07:36:11.056908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def build_prompt(user_input: str):\n",
    "    recent_history = conversation_history[-SLIDING_WINDOW:]\n",
    "    history_text = \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in recent_history])\n",
    "\n",
    "    fridge_lines = \"\\n\".join([f\"- {k}: {v}\" for k, v in fridge_state.items()])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "System: You are a smart kitchen assistant. Your goals:\n",
    "1. Suggest dinner using only the fridge items below.\n",
    "2. Update fridge state if the user mentions consuming or adding items.\n",
    "3. Be concise and helpful.\n",
    "\n",
    "Fridge Inventory:\n",
    "{fridge_lines}\n",
    "\n",
    "Conversation History:\n",
    "{history_text}\n",
    "\n",
    "User: {user_input}\n",
    "Assistant:\"\"\"\n",
    "    return prompt.strip()"
   ],
   "id": "f91c37d7f89fab6c",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.3 Chat Function",
   "id": "45886691a11e6468"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:36:13.564214Z",
     "start_time": "2025-08-22T07:36:13.560072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def chat(user_input: str, max_new_tokens: int = 128):\n",
    "    # Update fridge\n",
    "    fridge_update_msg = update_fridge(user_input)\n",
    "\n",
    "    # Build prompt\n",
    "    prompt = build_prompt(user_input)\n",
    "\n",
    "    # Encode prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode reply (skip the prompt portion)\n",
    "    reply = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Update conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    # Include fridge feedback\n",
    "    if fridge_update_msg:\n",
    "        reply = f\"{reply}\\n\\n[Fridge Update]: {fridge_update_msg}\"\n",
    "\n",
    "    return reply\n"
   ],
   "id": "ad4afb5993342f95",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:38:58.342747Z",
     "start_time": "2025-08-22T07:38:58.338194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chat_streaming(user_input, max_new_tokens=128):\n",
    "    # Update fridge state first\n",
    "    fridge_update_msg = update_fridge(user_input)\n",
    "\n",
    "    # Build prompt\n",
    "    prompt = build_prompt(user_input)\n",
    "\n",
    "    # Tokenize prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Configure generation\n",
    "    gen_config = GenerationConfig(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    # Generate token-by-token\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=gen_config\n",
    "    )\n",
    "\n",
    "    # Decode tokens as text\n",
    "    reply_text = tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    # Append to conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": reply_text})\n",
    "\n",
    "    # Combine reply + fridge update feedback\n",
    "    if fridge_update_msg:\n",
    "        reply_text = f\"{reply_text}\\n\\n[Fridge Update]: {fridge_update_msg}\"\n",
    "\n",
    "    return reply_text"
   ],
   "id": "a35fd643a1d31dfc",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Demo Conversation",
   "id": "63f89f7d0f66e2b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T07:38:23.310309Z",
     "start_time": "2025-08-22T07:36:15.632401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(chat(\"Hi! What's in my fridge?\"))\n",
    "print(chat(\"I ate 2 eggs.\"))\n",
    "print(chat(\"Can you suggest something for dinner?\"))\n",
    "print(chat(\"I bought more cheese.\"))\n",
    "print(chat(\"What can I cook tonight?\"))"
   ],
   "id": "7d6f5f918c647748",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[43], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHi! What\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43ms in my fridge?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(chat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI ate 2 eggs.\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(chat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan you suggest something for dinner?\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "Cell \u001B[1;32mIn[42], line 14\u001B[0m, in \u001B[0;36mchat\u001B[1;34m(user_input, max_new_tokens)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Generate output\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 14\u001B[0m     output_ids \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.7\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meos_token_id\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Decode reply (skip the prompt portion)\u001B[39;00m\n\u001B[0;32m     23\u001B[0m reply \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(output_ids[\u001B[38;5;241m0\u001B[39m][input_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]:], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mstrip()\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 120\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2617\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001B[0m\n\u001B[0;32m   2609\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   2610\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   2611\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[0;32m   2612\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   2613\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2614\u001B[0m     )\n\u001B[0;32m   2616\u001B[0m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[1;32m-> 2617\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2618\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2619\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2620\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2621\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2622\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2623\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2624\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2625\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2627\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[0;32m   2628\u001B[0m     \u001B[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001B[39;00m\n\u001B[0;32m   2629\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   2630\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   2631\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[0;32m   2632\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   2633\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2634\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3601\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[0;32m   3599\u001B[0m     is_prefill \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   3600\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3601\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   3603\u001B[0m \u001B[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001B[39;00m\n\u001B[0;32m   3604\u001B[0m model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_model_kwargs_for_generation(\n\u001B[0;32m   3605\u001B[0m     outputs,\n\u001B[0;32m   3606\u001B[0m     model_kwargs,\n\u001B[0;32m   3607\u001B[0m     is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   3608\u001B[0m )\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\transformers\\models\\gptj\\modeling_gptj.py:961\u001B[0m, in \u001B[0;36mGPTJForCausalLM.forward\u001B[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001B[0m\n\u001B[0;32m    949\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    950\u001B[0m \u001B[38;5;124;03minputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_dim)`, *optional*):\u001B[39;00m\n\u001B[0;32m    951\u001B[0m \u001B[38;5;124;03m    Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    957\u001B[0m \u001B[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001B[39;00m\n\u001B[0;32m    958\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    959\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m--> 961\u001B[0m transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    962\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    963\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    964\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    965\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    966\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    967\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    968\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    969\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    970\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    971\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    972\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    973\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    974\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    975\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m transformer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    977\u001B[0m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\transformers\\models\\gptj\\modeling_gptj.py:716\u001B[0m, in \u001B[0;36mGPTJModel.forward\u001B[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[0;32m    713\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states:\n\u001B[0;32m    714\u001B[0m     all_hidden_states \u001B[38;5;241m=\u001B[39m all_hidden_states \u001B[38;5;241m+\u001B[39m (hidden_states,)\n\u001B[1;32m--> 716\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    717\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    718\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    719\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    720\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    721\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    722\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    723\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    724\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    725\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    727\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    728\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:93\u001B[0m, in \u001B[0;36mGradientCheckpointingLayer.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     90\u001B[0m         logger\u001B[38;5;241m.\u001B[39mwarning(message)\n\u001B[0;32m     92\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs), \u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m---> 93\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\transformers\\models\\gptj\\modeling_gptj.py:460\u001B[0m, in \u001B[0;36mGPTJBlock.forward\u001B[1;34m(self, hidden_states, layer_past, attention_mask, position_ids, head_mask, use_cache, output_attentions, cache_position)\u001B[0m\n\u001B[0;32m    449\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_1(hidden_states)\n\u001B[0;32m    450\u001B[0m attn_outputs, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn(\n\u001B[0;32m    451\u001B[0m     hidden_states\u001B[38;5;241m=\u001B[39mhidden_states,\n\u001B[0;32m    452\u001B[0m     layer_past\u001B[38;5;241m=\u001B[39mlayer_past,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    458\u001B[0m     cache_position\u001B[38;5;241m=\u001B[39mcache_position,\n\u001B[0;32m    459\u001B[0m )\n\u001B[1;32m--> 460\u001B[0m feed_forward_hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    461\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m attn_outputs \u001B[38;5;241m+\u001B[39m feed_forward_hidden_states \u001B[38;5;241m+\u001B[39m residual\n\u001B[0;32m    463\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states, attn_weights\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\transformers\\models\\gptj\\modeling_gptj.py:422\u001B[0m, in \u001B[0;36mGPTJMLP.forward\u001B[1;34m(self, hidden_states)\u001B[0m\n\u001B[0;32m    421\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: Optional[torch\u001B[38;5;241m.\u001B[39mFloatTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mFloatTensor:\n\u001B[1;32m--> 422\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc_in\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    423\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact(hidden_states)\n\u001B[0;32m    424\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc_out(hidden_states)\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:565\u001B[0m, in \u001B[0;36mLinear4bit.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    562\u001B[0m \u001B[38;5;66;03m# IPEX CPU will change weight to 4D so don't need transpose\u001B[39;00m\n\u001B[0;32m    563\u001B[0m weight \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mt() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight\n\u001B[1;32m--> 565\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbnb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul_4bit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquant_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquant_state\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(inp_dtype)\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:452\u001B[0m, in \u001B[0;36mmatmul_4bit\u001B[1;34m(A, B, quant_state, out, bias)\u001B[0m\n\u001B[0;32m    450\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[0;32m    451\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 452\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mMatMul4Bit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquant_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m A\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m==\u001B[39m A\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m A\u001B[38;5;241m.\u001B[39mrequires_grad \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m A\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhpu\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    455\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m A\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m%\u001B[39m quant_state\u001B[38;5;241m.\u001B[39mblocksize \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\torch\\autograd\\function.py:576\u001B[0m, in \u001B[0;36mFunction.apply\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m    573\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[0;32m    574\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[0;32m    575\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[1;32m--> 576\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m    578\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[0;32m    579\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    580\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    581\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    582\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    583\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    584\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Cook-Bot\\.venv\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:380\u001B[0m, in \u001B[0;36mMatMul4Bit.forward\u001B[1;34m(ctx, A, B, out, bias, quant_state)\u001B[0m\n\u001B[0;32m    376\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mempty(A\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m B_shape[:\u001B[38;5;241m1\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mA\u001B[38;5;241m.\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mA\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    378\u001B[0m \u001B[38;5;66;03m# 1. Dequantize\u001B[39;00m\n\u001B[0;32m    379\u001B[0m \u001B[38;5;66;03m# 2. MatmulnN\u001B[39;00m\n\u001B[1;32m--> 380\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdequantize_4bit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquant_state\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    382\u001B[0m \u001B[38;5;66;03m# 3. Save state\u001B[39;00m\n\u001B[0;32m    383\u001B[0m ctx\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m quant_state\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-22T07:39:01.710644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(chat_streaming(\"Hi! What's in my fridge?\"))"
   ],
   "id": "afd95989d3547249",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'use_cache': False, 'bos_token_id': 50256, 'eos_token_id': 50256}. If this is not desired, please set these values explicitly.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
